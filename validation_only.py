# -*- coding: utf-8 -*-
"""validation_only.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w-6s0ycgPYGD8nLELLQjoLlqaW_kl0Q-

# Validaci√≥n de Modelos Entrenados - Detecci√≥n de C√°ncer Colorrectal

Este notebook contiene √∫nicamente el c√≥digo de validaci√≥n para los modelos ya entrenados.
"""

# Importaci√≥n de librer√≠as necesarias
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
import cv2
import os
from time import time
import math
from tensorflow.keras.utils import Sequence
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Librer√≠as importadas correctamente")
print(f"TensorFlow version: {tf.__version__}")

# Configuraci√≥n general
batch_size = 64
img_size = (224, 224)
class_names = ['ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM']
num_classes = len(class_names)

# Rutas de modelos entrenados
MODEL_PATHS = {
    'CNN_Simple': 'models/cnn_simple_model.h5',
    'ResNet50': 'models/resnet50.keras',
    'MobileNetV2': 'models/mobilenetv2_base_only.h5',
    'HybridAttention': 'models/Fast_HybridAttention_final.h5',
    'HybridAutoencoder': 'models/Fast_HybridAutoencoder_final.h5'
}

print("üìã Configuraci√≥n establecida:")
print(f"   - Tama√±o de imagen: {img_size}")
print(f"   - N√∫mero de clases: {num_classes}")
print(f"   - Clases: {class_names}")

# Generador de datos para validaci√≥n (sin data augmentation)
class ValidationDataGenerator(Sequence):
    def __init__(self, data_dir, batch_size=32, img_size=(224, 224), sample_size=1.0):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.img_size = img_size
        self.sample_size = sample_size

        # Obtener clases del directorio
        self.class_names = sorted(os.listdir(data_dir))

        # Listar todas las im√°genes
        self.image_paths = []
        self.labels = []

        for class_idx, class_name in enumerate(self.class_names):
            class_path = os.path.join(data_dir, class_name)
            if os.path.isdir(class_path):
                img_files = [f for f in os.listdir(class_path)
                           if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))]

                # Muestrear si es necesario
                sample_count = int(len(img_files) * self.sample_size)
                img_files = img_files[:sample_count]

                for img_name in img_files:
                    self.image_paths.append(os.path.join(class_path, img_name))
                    self.labels.append(class_idx)

        self.indices = np.arange(len(self.image_paths))
        print(f"üìä Dataset cargado: {len(self.image_paths)} im√°genes")

    def __len__(self):
        return math.ceil(len(self.image_paths) / self.batch_size)

    def __getitem__(self, index):
        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]
        X = []
        y = []

        for i in batch_indices:
            try:
                # Cargar imagen
                img = cv2.imread(self.image_paths[i])
                if img is None:
                    continue

                # Convertir BGR a RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Redimensionar
                img = cv2.resize(img, self.img_size)

                # Normalizar
                img = img / 255.0

                X.append(img)
                y.append(self.labels[i])
            except Exception as e:
                print(f"Error cargando imagen {self.image_paths[i]}: {e}")
                continue

        return np.array(X), np.array(y)
    
    def get_batch_for_model(self, index, target_size, apply_clahe=False):
        """Obtiene un batch redimensionado para un modelo espec√≠fico"""
        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]
        X = []
        y = []

        for i in batch_indices:
            try:
                # Cargar imagen
                img = cv2.imread(self.image_paths[i])
                if img is None:
                    continue

                # Convertir BGR a RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Aplicar CLAHE si se solicita
                if apply_clahe:
                    img = self.apply_clahe_preprocessing(img)

                # Redimensionar al tama√±o espec√≠fico del modelo
                img = cv2.resize(img, target_size)

                # Normalizar
                img = img / 255.0

                X.append(img)
                y.append(self.labels[i])
            except Exception as e:
                print(f"Error cargando imagen {self.image_paths[i]}: {e}")
                continue

        return np.array(X), np.array(y)
    
    def __getitem__(self, index, apply_clahe=False):
        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]
        X = []
        y = []

        for i in batch_indices:
            try:
                # Cargar imagen
                img = cv2.imread(self.image_paths[i])
                if img is None:
                    continue

                # Convertir BGR a RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Aplicar CLAHE si se solicita
                if apply_clahe:
                    img = self.apply_clahe_preprocessing(img)

                # Redimensionar
                img = cv2.resize(img, self.img_size)

                # Normalizar
                img = img / 255.0

                X.append(img)
                y.append(self.labels[i])
            except Exception as e:
                print(f"Error cargando imagen {self.image_paths[i]}: {e}")
                continue

        return np.array(X), np.array(y)
    
    def apply_clahe_preprocessing(self, img):
        """Aplica preprocesamiento CLAHE a la imagen"""
        # Convertir a LAB para aplicar CLAHE solo al canal L
        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
        
        # Crear objeto CLAHE
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        
        # Aplicar CLAHE al canal L
        lab[:, :, 0] = clahe.apply(lab[:, :, 0])
        
        # Convertir de vuelta a RGB
        img_clahe = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        
        return img_clahe

    def get_all_labels(self):
        """Obtiene todas las etiquetas verdaderas"""
        return np.array(self.labels)

    def get_class_distribution(self):
        """Devuelve la distribuci√≥n de clases"""
        unique, counts = np.unique(self.labels, return_counts=True)
        return dict(zip(unique, counts))

print("‚úÖ Generador de validaci√≥n definido")

# Cargar datos de validaci√≥n
# Ajusta esta ruta seg√∫n tu estructura de datos
DATA_DIR = 'CRC-VAL-HE-7K'  # Buscar en el directorio actual primero

# Intentar diferentes rutas posibles
possible_paths = [
    'CRC-VAL-HE-7K',
    '../CRC-VAL-HE-7K', 
    '../../CRC-VAL-HE-7K',
    '../../../CRC-VAL-HE-7K',
    'PRUEBAS',
    '../PRUEBAS',
    'data/CRC-VAL-HE-7K',
    'dataset/CRC-VAL-HE-7K'
]

DATA_DIR = None
for path in possible_paths:
    if os.path.exists(path):
        DATA_DIR = path
        print(f"‚úÖ Encontrado directorio de datos en: {path}")
        break

if DATA_DIR and os.path.exists(DATA_DIR):
    val_generator = ValidationDataGenerator(
        data_dir=DATA_DIR,
        batch_size=batch_size,
        img_size=img_size,
        sample_size=1  # Usar 10% de los datos para validaci√≥n r√°pida
    )

    print(f"üìà Distribuci√≥n de clases en validaci√≥n:")
    distribution = val_generator.get_class_distribution()
    for class_idx, count in distribution.items():
        print(f"   {val_generator.class_names[class_idx]}: {count} im√°genes")
else:
    print(f"‚ùå No se encontr√≥ ning√∫n directorio de datos v√°lido")
    print("üìÅ Directorios disponibles en el directorio actual:")
    try:
        for item in os.listdir('.'):
            if os.path.isdir(item):
                print(f"   - {item}/")
                # Verificar si contiene subdirectorios que podr√≠an ser clases
                try:
                    subdirs = [d for d in os.listdir(item) if os.path.isdir(os.path.join(item, d))]
                    if len(subdirs) > 0:
                        print(f"     Subdirectorios: {subdirs[:5]}{'...' if len(subdirs) > 5 else ''}")
                except:
                    pass
    except Exception as e:
        print(f"   Error listando directorios: {e}")
    
    print("\nüí° Sugerencias:")
    print("   1. Aseg√∫rate de que el directorio de datos est√© en la ubicaci√≥n correcta")
    print("   2. El directorio debe contener subdirectorios con nombres de clases")
    print("   3. Cada subdirectorio debe contener im√°genes (.png, .jpg, .jpeg, .tif, .tiff)")
    print("   4. Estructura esperada:")
    print("      CRC-VAL-HE-7K/")
    print("      ‚îú‚îÄ‚îÄ ADI/")
    print("      ‚îú‚îÄ‚îÄ BACK/") 
    print("      ‚îú‚îÄ‚îÄ DEB/")
    print("      ‚îî‚îÄ‚îÄ ...")

# Funci√≥n para cargar modelos
def load_model_safe(model_path, model_name):
    """Carga un modelo de forma segura"""
    try:
        if os.path.isdir(model_path):  # SavedModel format
            # Para Keras 3, usar TFSMLayer para modelos SavedModel
            print(f"‚ö†Ô∏è Cargando {model_name} como TFSMLayer (SavedModel format)")
            
            # Crear un wrapper para el modelo SavedModel
            class SavedModelWrapper(tf.keras.Model):
                def __init__(self, saved_model_path):
                    super().__init__()
                    self.tfsm_layer = tf.keras.layers.TFSMLayer(
                        saved_model_path, 
                        call_endpoint='serving_default'
                    )
                
                def call(self, inputs):
                    # El output del TFSMLayer puede tener diferentes formatos
                    result = self.tfsm_layer(inputs)
                    if isinstance(result, dict):
                        # Si es un diccionario, tomar el primer valor
                        return list(result.values())[0]
                    return result
                
                def count_params(self):
                    return sum([tf.size(var).numpy() for var in self.trainable_variables])
            
            model = SavedModelWrapper(model_path)
            
        else:  # .h5 format
            model = tf.keras.models.load_model(model_path)

        print(f"‚úÖ {model_name} cargado correctamente")
        try:
            params = model.count_params()
            print(f"   üìä Par√°metros: {params:,}")
        except:
            print(f"   üìä Par√°metros: No disponible para este formato")
        return model
    except Exception as e:
        print(f"‚ùå Error cargando {model_name}: {str(e)}")
        return None

# Cargar todos los modelos disponibles
loaded_models = {}

for model_name, model_path in MODEL_PATHS.items():
    if os.path.exists(model_path):
        model = load_model_safe(model_path, model_name)
        if model is not None:
            loaded_models[model_name] = model
    else:
        print(f"‚ö†Ô∏è No se encontr√≥ el modelo {model_name} en {model_path}")

print(f"\nüìã Modelos cargados exitosamente: {len(loaded_models)}")
for name in loaded_models.keys():
    print(f"   ‚úÖ {name}")

# Funci√≥n para validar un modelo
def validate_model(model, model_name, val_generator, apply_clahe=False):
    """Valida un modelo y devuelve m√©tricas detalladas"""
    clahe_suffix = " (con CLAHE)" if apply_clahe else ""
    print(f"\n{'='*50}")
    print(f"üîç Validando modelo: {model_name}{clahe_suffix}")
    print(f"{'='*50}")

    start_time = time()

    # Determinar el tama√±o de entrada requerido por el modelo
    try:
        input_shape = model.input_shape
        if isinstance(input_shape, list):
            input_shape = input_shape[0]  # Tomar la primera entrada si hay m√∫ltiples
        
        expected_height = input_shape[1] if input_shape[1] is not None else 224
        expected_width = input_shape[2] if input_shape[2] is not None else 224
        model_input_size = (expected_width, expected_height)
        
        print(f"üìê Tama√±o de entrada del modelo: {model_input_size}")
        if apply_clahe:
            print(f"üîß Aplicando preprocesamiento CLAHE")
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo determinar el tama√±o de entrada: {e}")
        print(f"üìê Usando tama√±o por defecto: (224, 224)")
        model_input_size = (224, 224)

    # Obtener predicciones
    print("üìä Generando predicciones...")
    predictions = []
    true_labels = []

    for i in range(len(val_generator)):
        try:
            # Obtener batch con el tama√±o correcto para este modelo
            if model_input_size != (224, 224):
                batch_x, batch_y = val_generator.get_batch_for_model(i, model_input_size, apply_clahe)
            else:
                # Para el m√©todo __getitem__ necesitamos una implementaci√≥n especial
                if apply_clahe:
                    batch_x, batch_y = get_batch_with_clahe(val_generator, i)
                else:
                    batch_x, batch_y = val_generator[i]
            
            if len(batch_x) > 0:
                batch_pred = model.predict(batch_x, verbose=0)
                
                # Manejar diferentes formatos de salida
                if isinstance(batch_pred, dict):
                    # Si es un diccionario, tomar el primer valor
                    batch_pred = list(batch_pred.values())[0]
                
                predictions.extend(batch_pred)
                true_labels.extend(batch_y)
        except Exception as e:
            print(f"‚ö†Ô∏è Error en batch {i}: {e}")
            continue

    if len(predictions) == 0:
        print("‚ùå No se pudieron obtener predicciones")
        return None

    predictions = np.array(predictions)
    true_labels = np.array(true_labels)
    predicted_labels = np.argmax(predictions, axis=1)

    # Calcular m√©tricas
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)

    # Top-3 accuracy
    if predictions.shape[1] >= 3:
        top3_predictions = tf.nn.top_k(predictions, k=3).indices.numpy()
        top3_accuracy = np.mean([true_labels[i] in top3_predictions[i] for i in range(len(true_labels))])
    else:
        top3_accuracy = accuracy  # Si hay menos de 3 clases, usar accuracy normal

    validation_time = time() - start_time

    # Mostrar resultados
    print(f"\nüìä RESULTADOS DE VALIDACI√ìN:")
    print(f"   üéØ Precisi√≥n (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   üéØ Top-3 Accuracy: {top3_accuracy:.4f} ({top3_accuracy*100:.2f}%)")
    print(f"   üìà Precisi√≥n (Precision): {precision:.4f}")
    print(f"   üìà Sensibilidad (Recall): {recall:.4f}")
    print(f"   üìà F1-Score: {f1:.4f}")
    print(f"   ‚è±Ô∏è Tiempo de validaci√≥n: {validation_time:.2f} segundos")
    print(f"   üìä Muestras evaluadas: {len(true_labels)}")

    # Reporte detallado por clase
    print(f"\nüìã REPORTE DETALLADO POR CLASE:")
    try:
        report = classification_report(
            true_labels,
            predicted_labels,
            target_names=class_names,
            digits=4,
            zero_division=0
        )
        print(report)
    except Exception as e:
        print(f"‚ö†Ô∏è Error generando reporte detallado: {e}")
        report = "No disponible"

    # Matriz de confusi√≥n
    cm = confusion_matrix(true_labels, predicted_labels)
    
    # Mostrar matriz de confusi√≥n en terminal (n√∫meros absolutos)
    print(f"\nüìä MATRIZ DE CONFUSI√ìN (N√∫meros Absolutos):")
    print("=" * 80)
    
    # Encabezados
    header = "Verdadero\\Predicho" + "".join(f"{name:>8}" for name in class_names) + "  Total"
    print(header)
    print("-" * len(header))
    
    # Filas de la matriz con n√∫meros absolutos
    for i, true_class in enumerate(class_names):
        row = f"{true_class:<15}"
        for j in range(len(class_names)):
            row += f"{cm[i,j]:>8d}"
        row += f"{cm[i].sum():>7d}"
        print(row)
    
    # Fila de totales
    total_row = "Total" + " " * 10
    for j in range(len(class_names)):
        total_row += f"{cm[:,j].sum():>8d}"
    total_row += f"{cm.sum():>7d}"
    print("-" * len(header))
    print(total_row)
    print("=" * 80)
    
    # Matriz normalizada por clase (porcentajes)
    print(f"\nüìä MATRIZ DE CONFUSI√ìN (Porcentajes por Clase Real):")
    print("=" * 80)
    
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Encabezados para porcentajes
    header_pct = "Verdadero\\Predicho" + "".join(f"{name:>8}" for name in class_names) + "  Total"
    print(header_pct)
    print("-" * len(header_pct))
    
    # Filas de la matriz con porcentajes
    for i, true_class in enumerate(class_names):
        row = f"{true_class:<15}"
        for j in range(len(class_names)):
            percentage = cm_normalized[i,j] * 100
            row += f"{percentage:>7.1f}%"
        row += f" 100.0%"
        print(row)
    
    print("=" * 80)
    
    # Estad√≠sticas adicionales por clase
    print(f"\nüìà ESTAD√çSTICAS POR CLASE:")
    for i, class_name in enumerate(class_names):
        if cm[i].sum() > 0:
            class_accuracy = cm[i,i] / cm[i].sum()
            correct = cm[i,i]
            total = cm[i].sum()
            print(f"   {class_name}: {correct}/{total} = {class_accuracy:.4f} ({class_accuracy*100:.2f}%)")
        else:
            print(f"   {class_name}: Sin muestras")

    return {
        'model_name': model_name + clahe_suffix,
        'accuracy': accuracy,
        'top3_accuracy': top3_accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'validation_time': validation_time,
        'samples_evaluated': len(true_labels),
        'confusion_matrix': cm,
        'classification_report': report,
        'predictions': predictions,
        'true_labels': true_labels,
        'predicted_labels': predicted_labels
    }

# Funci√≥n auxiliar para obtener batch con CLAHE
def get_batch_with_clahe(val_generator, index):
    """Obtiene un batch aplicando CLAHE"""
    batch_indices = val_generator.indices[index*val_generator.batch_size:(index+1)*val_generator.batch_size]
    X = []
    y = []

    for i in batch_indices:
        try:
            # Cargar imagen
            img = cv2.imread(val_generator.image_paths[i])
            if img is None:
                continue

            # Convertir BGR a RGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # Aplicar CLAHE
            img = val_generator.apply_clahe_preprocessing(img)

            # Redimensionar
            img = cv2.resize(img, val_generator.img_size)

            # Normalizar
            img = img / 255.0

            X.append(img)
            y.append(val_generator.labels[i])
        except Exception as e:
            print(f"Error cargando imagen {val_generator.image_paths[i]}: {e}")
            continue

    return np.array(X), np.array(y)

print("‚úÖ Funci√≥n de validaci√≥n definida")

# Funci√≥n para generar matriz de confusi√≥n
def plot_confusion_matrix(cm, model_name, class_names):
    """Genera una matriz de confusi√≥n visualizada"""
    plt.figure(figsize=(10, 8))

    # Normalizar la matriz de confusi√≥n
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    sns.heatmap(cm_normalized,
                annot=True,
                fmt='.3f',
                cmap='Blues',
                xticklabels=class_names,
                yticklabels=class_names,
                cbar_kws={'label': 'Proporci√≥n normalizada'})

    plt.title(f'Matriz de Confusi√≥n Normalizada - {model_name}',
              fontsize=14, fontweight='bold')
    plt.xlabel('Predicci√≥n', fontsize=12)
    plt.ylabel('Verdadero', fontsize=12)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()

    # Guardar la imagen
    os.makedirs('reports', exist_ok=True)
    plt.savefig(f'reports/{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()

    return cm_normalized

print("‚úÖ Funci√≥n para matriz de confusi√≥n definida")

# VALIDACI√ìN DE TODOS LOS MODELOS
print("üöÄ INICIANDO VALIDACI√ìN DE TODOS LOS MODELOS")
print("="*60)

validation_results = {}

if 'val_generator' in locals() and len(loaded_models) > 0:
    # Validaci√≥n normal de todos los modelos
    for model_name, model in loaded_models.items():
        try:
            print(f"\nüîß Preparando validaci√≥n para {model_name}...")
            
            # Validar modelo sin CLAHE
            result = validate_model(model, model_name, val_generator, apply_clahe=False)
            
            if result is not None:
                validation_results[model_name] = result
                
                # Generar matriz de confusi√≥n
                plot_confusion_matrix(result['confusion_matrix'], model_name, class_names)
            else:
                print(f"‚ö†Ô∏è No se pudo validar {model_name} - saltando...")

        except Exception as e:
            print(f"‚ùå Error validando {model_name}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\n‚úÖ Validaci√≥n normal completada. Modelos validados exitosamente: {len(validation_results)}")
    
    # Pregunta opcional para validaci√≥n con CLAHE
    print("\n" + "="*80)
    print("üî¨ VALIDACI√ìN OPCIONAL CON PREPROCESAMIENTO CLAHE")
    print("="*80)
    print("CLAHE (Contrast Limited Adaptive Histogram Equalization) puede mejorar")
    print("el contraste y la visibilidad de caracter√≠sticas en im√°genes m√©dicas.")
    print()
    
    # Verificar qu√© modelos est√°n disponibles para CLAHE
    clahe_models = {}
    if 'ResNet50' in loaded_models:
        clahe_models['ResNet50'] = loaded_models['ResNet50']
    
    if clahe_models:
        print(f"Modelos disponibles para validaci√≥n con CLAHE: {list(clahe_models.keys())}")
        
        while True:
            answer = input("\n¬øDesea validar ResNet50 y MobileNetV2 con preprocesamiento CLAHE? (s/n): ").lower().strip()
            if answer in ['s', 'si', 's√≠', 'y', 'yes']:
                print("\nüî¨ Iniciando validaci√≥n con CLAHE...")
                
                for model_name, model in clahe_models.items():
                    try:
                        print(f"\nüîß Preparando validaci√≥n CLAHE para {model_name}...")
                        
                        # Validar modelo con CLAHE
                        result_clahe = validate_model(model, model_name, val_generator, apply_clahe=True)
                        
                        if result_clahe is not None:
                            validation_results[model_name + " (con CLAHE)"] = result_clahe
                            
                            # Generar matriz de confusi√≥n
                            plot_confusion_matrix(result_clahe['confusion_matrix'], 
                                                model_name + " (con CLAHE)", class_names)
                        else:
                            print(f"‚ö†Ô∏è No se pudo validar {model_name} con CLAHE - saltando...")

                    except Exception as e:
                        print(f"‚ùå Error validando {model_name} con CLAHE: {str(e)}")
                        import traceback
                        traceback.print_exc()
                        continue
                
                print(f"\n‚úÖ Validaci√≥n con CLAHE completada.")
                break
                
            elif answer in ['n', 'no']:
                print("‚ö†Ô∏è Saltando validaci√≥n con CLAHE.")
                break
            else:
                print("‚ö†Ô∏è Por favor responda 's' para s√≠ o 'n' para no.")
    else:
        print("‚ö†Ô∏è No hay modelos ResNet50 o MobileNetV2 disponibles para validaci√≥n con CLAHE.")
    
    print(f"\n‚úÖ Validaci√≥n total completada. Modelos validados: {len(validation_results)}")
else:
    print("‚ùå No hay modelos cargados o datos de validaci√≥n disponibles")
    if 'val_generator' not in locals():
        print("   - No se pudo cargar el generador de datos de validaci√≥n")
    if len(loaded_models) == 0:
        print("   - No se cargaron modelos correctamente")

# RESUMEN COMPARATIVO DE TODOS LOS MODELOS
if validation_results:
    print("\n" + "="*80)
    print("üèÜ RESUMEN COMPARATIVO DE VALIDACI√ìN")
    print("="*80)

    # Crear DataFrame con resultados
    summary_data = []
    for model_name, result in validation_results.items():
        summary_data.append({
            'Modelo': model_name,
            'Accuracy': f"{result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)",
            'Top-3 Acc': f"{result['top3_accuracy']:.4f} ({result['top3_accuracy']*100:.2f}%)",
            'Precision': f"{result['precision']:.4f}",
            'Recall': f"{result['recall']:.4f}",
            'F1-Score': f"{result['f1_score']:.4f}",
            'Tiempo (s)': f"{result['validation_time']:.2f}",
            'Muestras': result['samples_evaluated']
        })

    df_summary = pd.DataFrame(summary_data)

    print("\nüìä TABLA COMPARATIVA:")
    print(df_summary.to_string(index=False))

    # Encontrar el mejor modelo
    best_model = max(validation_results.items(), key=lambda x: x[1]['accuracy'])
    best_name, best_result = best_model

    print(f"\nü•á MEJOR MODELO: {best_name}")
    print(f"   üéØ Mejor Accuracy: {best_result['accuracy']:.4f} ({best_result['accuracy']*100:.2f}%)")
    print(f"   üéØ Top-3 Accuracy: {best_result['top3_accuracy']:.4f} ({best_result['top3_accuracy']*100:.2f}%)")
    print(f"   üìà F1-Score: {best_result['f1_score']:.4f}")

    # Guardar resultados
    df_summary.to_csv('reports/validation_summary.csv', index=False)
    print(f"\nüíæ Resultados guardados en: reports/validation_summary.csv")

    # Gr√°fico comparativo
    plt.figure(figsize=(12, 8))

    models = list(validation_results.keys())
    accuracies = [validation_results[model]['accuracy'] for model in models]
    f1_scores = [validation_results[model]['f1_score'] for model in models]

    x = np.arange(len(models))
    width = 0.35

    plt.subplot(1, 2, 1)
    bars1 = plt.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8, color='skyblue')
    bars2 = plt.bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8, color='lightcoral')

    plt.xlabel('Modelos')
    plt.ylabel('Score')
    plt.title('Comparaci√≥n de Modelos - Accuracy vs F1-Score')
    plt.xticks(x, models, rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)

    # A√±adir valores en las barras
    for bar in bars1:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)

    for bar in bars2:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)

    # Top-3 Accuracy
    plt.subplot(1, 2, 2)
    top3_accs = [validation_results[model]['top3_accuracy'] for model in models]
    bars3 = plt.bar(models, top3_accs, alpha=0.8, color='lightgreen')

    plt.xlabel('Modelos')
    plt.ylabel('Top-3 Accuracy')
    plt.title('Top-3 Accuracy por Modelo')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    for bar in bars3:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)

    plt.tight_layout()
    plt.savefig('reports/models_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

else:
    print("‚ùå No hay resultados de validaci√≥n para mostrar")

